{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2501c050-4b9f-457f-88db-5cd6ce2735ed",
   "metadata": {},
   "source": [
    "# Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf43695-316f-4986-a00b-f116daf83703",
   "metadata": {},
   "source": [
    "For a model, such as a **Large Language Model (LLM)**, to be able to read data, the data must first be transformed into some format the model can process and \"understand\". The models do not comprehend raw input data directly, various steps happen \"under the hood\" to make the input usable. This notebook will explore it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f416354-0001-4027-ac11-09ee3ff6b84d",
   "metadata": {},
   "source": [
    "## What is tokenization?\n",
    "\n",
    "Tokenization is the process of converting text into smaller units called tokens (often subwords) and mapping them to numeric IDs so models can process them, then reversing the process to produce text. Tokenizers are the first and last step around the model: encode text to tokens, embed/process, then decode tokens back to text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8706229-d80e-4315-ae4e-0a578282272b",
   "metadata": {},
   "source": [
    "## Real-world impact\n",
    "\n",
    "**Cost**: Most API pricing is per token (or, more precisely, per million tokens), so more tokens mean higher cost. Typically, input data (the data we enter), cached input data (previously processed inputs stored for faster access), and output data (the data produced by the model) are billed differently.\n",
    "\n",
    "**Performance and limits**: Tokenization determines how many tokens a prompt becomes, affecting speed, attention cost, and hitting context limits.\n",
    "\n",
    "**Practical control**: Optimizing prompts and model choice can reduce token counts and costs without losing necessary context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719f3e72-f138-4a71-9b1e-d687b2dec6aa",
   "metadata": {},
   "source": [
    "## Rule of thumb\n",
    "\n",
    "There is an accepted rule that in English, 1 token is usually about 4 characters, or roughly 3/4 of a word. Therefore, it can be assumed that 100 tokens are about 75 words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f69d2d-b1d5-4715-bd66-d830221631fb",
   "metadata": {},
   "source": [
    "## Main tokenization algorithms in Natural Language Processing (NLP) models\n",
    "\n",
    "- **Whitespace / Rule-based** - split on spaces and punctuation, often using simple rules (like how to treat quotes or hyphens). Fast and easy to implement, but can break down with tricky language cases or different writing styles.\n",
    "\n",
    "- **Word tokenization** - uses language-specific rules to split text into words. Good for Western languages like English, but struggles with languages without spaces, like Chinese or Japanese, and for contractions/punctuation quirks.\n",
    "\n",
    "- **Sentence tokenization** - identifies where sentences start and end, which is usually based on punctuation and common abbreviations. Often used as a first step before splitting text into words or subwords.\n",
    "\n",
    "- **Character tokenization** - each character is a separate token. It avoids Out-of-Vocabulary issues but results in longer sequences and weaker semantics per token.\n",
    "\n",
    "- **Subword tokenization**\n",
    "\n",
    "  - **Byte-Pair Enconding (BPE)** - starts with characters and repeatedly merges the most frequent pairs to build a vocabulary. It is efficient and consistent but can create unusual splits for rare or made-up words.\n",
    "\n",
    "  - **WordPiece** - similar to BPE, but instead of just merging the most frequent pairs, it chooses merges that maximize the likelihood of the training data under a language model. It is used in models like **BERT** and usually produces more linguistically meaningful subwords. In many implementations, continuation subwords are marked with a prefix like ‚Äú##‚Äù (e.g., ‚Äúplay‚Äù + ‚Äú##ing‚Äù), whereas **BPE** does not require such a convention.\n",
    " \n",
    "  - **Unigram** - starts with a large set of possible subword tokens and gradually removes less useful ones, keeping the vocabulary that best explains the training data with the lowest loss. Unlike **BPE** or **WordPiece**, it can generate multiple valid segmentations for the same word and chooses the most likely one. It also supports sampling, which helps make models more robust during training (a technique called subword regularization).\n",
    " \n",
    "  - **SentencePiece** - a framework that trains tokenizers (like **BPE** or **Unigram**) directly on raw text (no pre-tokenization), commonly marking word starts with a special token (e.g., ‚ñÅ (visual symbol - not an underscore)) - good for languages that don't uses spaces.\n",
    "\n",
    "- **Byte-level variants**: **GPT** models use **byte-level BPE** via OpenAI encodings like cl100k_base, operating on raw bytes to improve Unicode robustness and compression, avoid OOVs, and ensure consistent behavior across scripts and emoji."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29401885-d976-4e78-ae64-8d197b7c0bcd",
   "metadata": {},
   "source": [
    "### Whitespace / Rule-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fcbd471-b262-4547-bdeb-c43841c9408a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"don't\", 'like', 'the', 'so-called', \"'state-of-the-art'\", 'design', 'she', 'said', \"It's\", 'too', 'over-the-top', 'for', 'my', 'taste', 'especially', 'in', 'the', 'U', 'S']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "text = \"I don't like the so-called 'state-of-the-art' design, she said. It's too over-the-top for my taste, especially in the U.S.\"\n",
    "tokens = [t for t in re.split(r\"[^\\w'-]+\", text) if t]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73093dd-7956-417a-8177-0ed249ff4276",
   "metadata": {},
   "source": [
    "### Word tokenization (spaCy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0b47469-267c-4e64-9cca-5959f6beb8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'do', \"n't\", 'like', 'the', 'so', '-', 'called', \"'\", 'state', '-', 'of', '-', 'the', '-', 'art', \"'\", 'design', ',', 'she', 'said', '.', 'It', \"'s\", 'too', 'over', '-', 'the', '-', 'top', 'for', 'my', 'taste', ',', 'especially', 'in', 'the', 'U.S.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "text = \"I don't like the so-called 'state-of-the-art' design, she said. It's too over-the-top for my taste, especially in the U.S.\"\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "tokens = [t.text for t in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0155ac6d-0edf-472c-85e2-4608b459c8ac",
   "metadata": {},
   "source": [
    "### Sentence tokenization (NLTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dce8632-235b-4e03-b71c-f2e01768b937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dr. Smith arrived at 5 p.m.', 'He left at 6.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Dr. Smith arrived at 5 p.m. He left at 6.\"\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3078361-4b96-45e6-bb7b-636080c3f11f",
   "metadata": {},
   "source": [
    "### Character tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad5afeb7-f6e5-4a29-9a87-3d4d5635531a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', ' ', 's', 'i', 'm', 'p', 'l', 'e', ' ', 't', 'e', 'x', 't', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"A simple text.\"\n",
    "tokens = list(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094f8e6c-4abc-4464-a4c7-f14dbbc49c8c",
   "metadata": {},
   "source": [
    "### Byte-Pair Encoding (BPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ddf2baf-fa67-459d-ad7f-829f954bcd5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned vocabulary:\n",
      "['!', '-', '.', 'D', 'Dr', 'S', 'Sm', '[CLS]', '[MASK]', '[PAD]', '[SEP]', '[UNK]', 'a', 'ab', 'ar', 'as', 'at', 'b', 'be', 'c', 'cas', 'e', 'en', 'er', 'ev', 'f', 'fun', 'g', 'h', 'i', 'iev', 'in', 'io', 'is', 'ith', 'iz', 'k', 'l', 'm', 'n', 'o', 'r', 's', 't', 'th', 'u', 'un', 'v', 'w', 'z']\n",
      "\n",
      "Encoding the word 'lowercasing':\n",
      "Tokens: ['l', 'o', 'w', 'er', 'cas', 'in', 'g']\n",
      "Token IDs: [19, 22, 28, 41, 39, 44, 15]\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "\n",
    "corpus = [\n",
    "    \"lowercasing\", \n",
    "    \"unbelievable\", \n",
    "    \"state-of-the-art\", \n",
    "    \"tokenization is fun!\"\n",
    "    \"Dr. Smith\"\n",
    "]\n",
    "\n",
    "# initialize a BPE tokenizer with unknown token\n",
    "tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
    "\n",
    "# use whitespace to split text initially (pre-tokenization)\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "# trainer with small vocab size for demo, plus special tokens\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=50,  # small vocab for demo clarity\n",
    "    special_tokens=[\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    ")\n",
    "\n",
    "# train the tokenizer on the toy corpus\n",
    "tokenizer.train_from_iterator(corpus, trainer)\n",
    "\n",
    "# show learned vocabulary tokens\n",
    "print(\"Learned vocabulary:\")\n",
    "print(sorted(tokenizer.get_vocab().keys()))\n",
    "\n",
    "# encode a sample word and show the tokens and IDs\n",
    "sample = \"lowercasing\"\n",
    "encoded = tokenizer.encode(sample)\n",
    "\n",
    "print(f\"\\nEncoding the word '{sample}':\")\n",
    "print(\"Tokens:\", encoded.tokens)\n",
    "print(\"Token IDs:\", encoded.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18829feb-841e-4171-9b80-5ffd3e77e17f",
   "metadata": {},
   "source": [
    "### WordPiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56d99bf3-e03c-4842-9b79-d625724d41ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned vocabulary:\n",
      "['!', '##a', '##b', '##c', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##r', '##s', '##t', '##u', '##v', '##w', '##z', '-', '.', 'D', 'S', '[CLS]', '[MASK]', '[PAD]', '[SEP]', '[UNK]', 'a', 'b', 'c', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'r', 's', 't', 'u', 'v', 'w', 'z']\n",
      "\n",
      "Encoding the word 'lowercasing':\n",
      "Tokens: ['l', '##o', '##w', '##e', '##r', '##c', '##a', '##s', '##i', '##n', '##g']\n",
      "Token IDs: [19, 35, 36, 32, 37, 38, 31, 39, 40, 34, 41]\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"lowercasing\", \n",
    "    \"unbelievable\", \n",
    "    \"state-of-the-art\", \n",
    "    \"tokenization is fun!\"\n",
    "    \"Dr. Smith\"\n",
    "]\n",
    "\n",
    "# initialize a BPE tokenizer with unknown token\n",
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
    "\n",
    "# use whitespace to split text initially (pre-tokenization)\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "# trainer with small vocab size for demo, plus special tokens\n",
    "trainer = trainers.WordPieceTrainer(\n",
    "    vocab_size=50,  # small vocab for demo clarity\n",
    "    special_tokens=[\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    ")\n",
    "\n",
    "# train the tokenizer on the toy corpus\n",
    "tokenizer.train_from_iterator(corpus, trainer)\n",
    "\n",
    "# show learned vocabulary tokens\n",
    "print(\"Learned vocabulary:\")\n",
    "print(sorted(tokenizer.get_vocab().keys()))\n",
    "\n",
    "# encode a sample word and show the tokens and IDs\n",
    "sample = \"lowercasing\"\n",
    "encoded = tokenizer.encode(sample)\n",
    "\n",
    "print(f\"\\nEncoding the word '{sample}':\")\n",
    "print(\"Tokens:\", encoded.tokens)\n",
    "print(\"Token IDs:\", encoded.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f147afbf-75f3-4ba6-8637-bce5fc7835ab",
   "metadata": {},
   "source": [
    "### Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbe7795e-10b2-4f13-aaa1-2342f9cd38c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned vocabulary:\n",
      "['<unk>', '<s>', '</s>', '‚ñÅ', 'e', 'i', 'o', 't', 'a', 'n', '-', 's', 'r', 'l', 'b', 'f', 'th', 'un', 'at', 'u', '!', '.', 'D', 'S', 'g', 'k', 'm', 'z', 'v', 'w', 'c', 'h']\n",
      "\n",
      "Encoding the word 'lowercasing':\n",
      "Tokens: ['‚ñÅ', 'l', 'o', 'w', 'e', 'r', 'c', 'a', 's', 'i', 'n', 'g']\n",
      "Token IDs: [3, 13, 6, 29, 4, 12, 30, 8, 11, 5, 9, 24]\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "import tempfile\n",
    "\n",
    "# corupus text\n",
    "corpus_text = \"lowercasing\\nunbelievable\\nstate-of-the-art\\ntokenization is fun!\\nDr. Smith\\n\"\n",
    "\n",
    "# create a temporary file and write the corpus text there\n",
    "with tempfile.NamedTemporaryFile(mode='w', delete=False) as tmp_file:\n",
    "    tmp_file.write(corpus_text)\n",
    "    tmp_filename = tmp_file.name\n",
    "\n",
    "# train the unigram model using the temp file\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input=tmp_filename,\n",
    "    model_prefix=\"spm_unigram\",\n",
    "    vocab_size=32,\n",
    "    model_type=\"unigram\",\n",
    "    character_coverage=1.0\n",
    ")\n",
    "\n",
    "# load and use the model\n",
    "sp = spm.SentencePieceProcessor(model_file=\"spm_unigram.model\")\n",
    "\n",
    "sample = \"lowercasing\"\n",
    "\n",
    "print(\"Learned vocabulary:\")\n",
    "vocab_list = [sp.id_to_piece(i) for i in range(sp.get_piece_size())]\n",
    "print(vocab_list)\n",
    "\n",
    "print(f\"\\nEncoding the word '{sample}':\")\n",
    "\n",
    "tokens = sp.encode(sample, out_type=str)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# encode with token IDs (integers)\n",
    "token_ids = sp.encode(\"lowercasing\", out_type=int)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "\n",
    "# delete the temporary file\n",
    "import os\n",
    "os.remove(tmp_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab82601-810f-44a5-a6a5-3edec87f71e9",
   "metadata": {},
   "source": [
    "**NOTE**: Results from subword models depend heavily on the training corpus and chosen hyperparameters. Therefore, variations in training data or settings can produce different tokenization results.\n",
    "\n",
    "This is why pretrained tokenizers, trained on large diverse corpora with carefully tuned hyperparameters, are commonly used - they provide consistent and reliable tokenization across many tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa989cc-6fd5-4264-b4e1-a827d0bb7f2f",
   "metadata": {},
   "source": [
    "## Pretrained tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "833a6b4a-744b-4eeb-8b1d-998ac63547d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\n",
    "    \"Natural Language Processing (NLP) enables computers to understand human language. \"\n",
    "    \"Challenges include handling idioms like 'kick the bucket', contractions such as 'won't', \"\n",
    "    \"abbreviations like 'e.g.', hyphenated words like 'well-known', and even emojis ü§ñ. \"\n",
    "    \"Context is key: 'bank' in 'river bank' vs. 'bank' in 'savings bank'. \"\n",
    "    \"Also, numbers (e.g., 42, 3.1415) and proper nouns like 'New York' add complexity.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f745b43-92e9-4b0d-95fe-d4ac4e701eab",
   "metadata": {},
   "source": [
    "### BERT (WordPiece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "598fd171-ff17-4555-9d87-b5a6c76c5553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', '(', 'nl', '##p', ')', 'enables', 'computers', 'to', 'understand', 'human', 'language', '.', 'challenges', 'include', 'handling', 'id', '##ioms', 'like', \"'\", 'kick', 'the', 'bucket', \"'\", ',', 'contraction', '##s', 'such', 'as', \"'\", 'won', \"'\", 't', \"'\", ',', 'abbreviation', '##s', 'like', \"'\", 'e', '.', 'g', '.', \"'\", ',', 'h', '##yp', '##hen', '##ated', 'words', 'like', \"'\", 'well', '-', 'known', \"'\", ',', 'and', 'even', 'em', '##oj', '##is', '[UNK]', '.', 'context', 'is', 'key', ':', \"'\", 'bank', \"'\", 'in', \"'\", 'river', 'bank', \"'\", 'vs', '.', \"'\", 'bank', \"'\", 'in', \"'\", 'savings', 'bank', \"'\", '.', 'also', ',', 'numbers', '(', 'e', '.', 'g', '.', ',', '42', ',', '3', '.', '141', '##5', ')', 'and', 'proper', 'nouns', 'like', \"'\", 'new', 'york', \"'\", 'add', 'complexity', '.']\n",
      "torch.Size([1, 117])\n",
      "torch.Size([1, 117, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "name = \"bert-base-uncased\"\n",
    "tok = AutoTokenizer.from_pretrained(name)\n",
    "model = AutoModel.from_pretrained(name)\n",
    "\n",
    "enc = tok(text, return_tensors=\"pt\")\n",
    "print(tok.tokenize(text))\n",
    "print(enc[\"input_ids\"].shape) # [1, seq_len]\n",
    "out = model(**enc) # out.last_hidden_state: [1, seq_len, hidden]\n",
    "print(out.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eedd66-9d56-42ec-acd9-fe4eee6835f6",
   "metadata": {},
   "source": [
    "### RoBERTa (BPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "349bab0a-bd4b-471b-9f14-b6fc748bea49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'ƒ†Language', 'ƒ†Processing', 'ƒ†(', 'N', 'LP', ')', 'ƒ†enables', 'ƒ†computers', 'ƒ†to', 'ƒ†understand', 'ƒ†human', 'ƒ†language', '.', 'ƒ†Challenges', 'ƒ†include', 'ƒ†handling', 'ƒ†idi', 'oms', 'ƒ†like', \"ƒ†'\", 'kick', 'ƒ†the', 'ƒ†bucket', \"',\", 'ƒ†contract', 'ions', 'ƒ†such', 'ƒ†as', \"ƒ†'\", 'won', \"'t\", \"',\", 'ƒ†abbrevi', 'ations', 'ƒ†like', \"ƒ†'\", 'e', '.', 'g', \".'\", ',', 'ƒ†hyp', 'hen', 'ated', 'ƒ†words', 'ƒ†like', \"ƒ†'\", 'well', '-', 'known', \"',\", 'ƒ†and', 'ƒ†even', 'ƒ†em', 'oj', 'is', 'ƒ†√∞≈Å', '¬§', 'ƒ∏', '.', 'ƒ†Context', 'ƒ†is', 'ƒ†key', ':', \"ƒ†'\", 'bank', \"'\", 'ƒ†in', \"ƒ†'\", 'river', 'ƒ†bank', \"'\", 'ƒ†vs', '.', \"ƒ†'\", 'bank', \"'\", 'ƒ†in', \"ƒ†'\", 'sav', 'ings', 'ƒ†bank', \"'.\", 'ƒ†Also', ',', 'ƒ†numbers', 'ƒ†(', 'e', '.', 'g', '.,', 'ƒ†42', ',', 'ƒ†3', '.', '14', '15', ')', 'ƒ†and', 'ƒ†proper', 'ƒ†noun', 's', 'ƒ†like', \"ƒ†'\", 'New', 'ƒ†York', \"'\", 'ƒ†add', 'ƒ†complexity', '.']\n",
      "torch.Size([1, 113])\n",
      "torch.Size([1, 113, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "name = \"roberta-base\"\n",
    "tok = AutoTokenizer.from_pretrained(name)\n",
    "model = AutoModel.from_pretrained(name)\n",
    "\n",
    "enc = tok(text, return_tensors=\"pt\")\n",
    "print(tok.tokenize(text)) # BPE (byte-levelish pretokenization with ƒ† marker)\n",
    "print(enc[\"input_ids\"].shape) # [1, seq_len]\n",
    "out = model(**enc) # out.last_hidden_state: [1, seq_len, hidden]\n",
    "print(out.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77687a0-ce20-413e-90cc-16b978f4bd5d",
   "metadata": {},
   "source": [
    "### GPT (Byte-level BPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83615793-c33f-4601-9374-214e67708190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'ƒ†Language', 'ƒ†Processing', 'ƒ†(', 'N', 'LP', ')', 'ƒ†enables', 'ƒ†computers', 'ƒ†to', 'ƒ†understand', 'ƒ†human', 'ƒ†language', '.', 'ƒ†Challenges', 'ƒ†include', 'ƒ†handling', 'ƒ†idi', 'oms', 'ƒ†like', \"ƒ†'\", 'kick', 'ƒ†the', 'ƒ†bucket', \"',\", 'ƒ†contract', 'ions', 'ƒ†such', 'ƒ†as', \"ƒ†'\", 'won', \"'t\", \"',\", 'ƒ†abbrevi', 'ations', 'ƒ†like', \"ƒ†'\", 'e', '.', 'g', \".'\", ',', 'ƒ†hyp', 'hen', 'ated', 'ƒ†words', 'ƒ†like', \"ƒ†'\", 'well', '-', 'known', \"',\", 'ƒ†and', 'ƒ†even', 'ƒ†em', 'oj', 'is', 'ƒ†√∞≈Å', '¬§', 'ƒ∏', '.', 'ƒ†Context', 'ƒ†is', 'ƒ†key', ':', \"ƒ†'\", 'bank', \"'\", 'ƒ†in', \"ƒ†'\", 'river', 'ƒ†bank', \"'\", 'ƒ†vs', '.', \"ƒ†'\", 'bank', \"'\", 'ƒ†in', \"ƒ†'\", 'sav', 'ings', 'ƒ†bank', \"'.\", 'ƒ†Also', ',', 'ƒ†numbers', 'ƒ†(', 'e', '.', 'g', '.,', 'ƒ†42', ',', 'ƒ†3', '.', '14', '15', ')', 'ƒ†and', 'ƒ†proper', 'ƒ†noun', 's', 'ƒ†like', \"ƒ†'\", 'New', 'ƒ†York', \"'\", 'ƒ†add', 'ƒ†complexity', '.']\n",
      "torch.Size([1, 111])\n",
      "torch.Size([1, 111, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Language Processing (NLP) enables computers to understand human language. Challenges include handling idioms like 'kick the bucket', contractions such as 'won't', abbreviations like 'e.g.', hyphenated words like 'well-known', and even emojis ü§ñ. Context is key: 'bank' in 'river bank' vs. 'bank' in 'savings bank'. Also, numbers (e.g., 42, 3.1415) and proper nouns like 'New York' add complexity.\n",
      "\n",
      "The NLP is a powerful tool for understanding human language. It is a tool for understanding\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "\n",
    "name = \"gpt2\"\n",
    "tok = AutoTokenizer.from_pretrained(name)\n",
    "model = AutoModel.from_pretrained(name)\n",
    "\n",
    "enc = tok(text, return_tensors=\"pt\")\n",
    "print(tok.tokenize(text)) # byte-level BPE\n",
    "print(enc[\"input_ids\"].shape) # [1, seq_len]\n",
    "out = model(**enc) # out.last_hidden_state: [1, seq_len, hidden]\n",
    "print(out.last_hidden_state.shape)\n",
    "\n",
    "gen_model = AutoModelForCausalLM.from_pretrained(name)\n",
    "gen = gen_model.generate(**enc, max_new_tokens=20)\n",
    "print(tok.decode(gen[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e824fb3b-4e0f-4ff3-9f58-26fde38f83b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', ' Language', ' Processing', ' (', 'N', 'LP', ')', ' enables', ' computers', ' to', ' understand', ' human', ' language', '.', ' Challenges', ' include', ' handling', ' idi', 'oms', ' like', \" '\", 'kick', ' the', ' bucket', \"',\", ' contr', 'actions', ' such', ' as', \" '\", 'won', \"'t\", \"',\", ' abbrev', 'iations', ' like', \" '\", 'e', '.g', \".',\", ' hy', 'phen', 'ated', ' words', ' like', \" '\", 'well', '-known', \"',\", ' and', ' even', ' emojis', ' ÔøΩ', 'ÔøΩ', 'ÔøΩ', '.', ' Context', ' is', ' key', ':', \" '\", 'bank', \"'\", ' in', \" '\", 'river', ' bank', \"'\", ' vs', '.', \" '\", 'bank', \"'\", ' in', \" '\", 's', 'avings', ' bank', \"'.\", ' Also', ',', ' numbers', ' (', 'e', '.g', '.,', ' ', '42', ',', ' ', '3', '.', '141', '5', ')', ' and', ' proper', ' nouns', ' like', \" '\", 'New', ' York', \"'\", ' add', ' complexity', '.']\n",
      "[55381, 11688, 29225, 320, 45, 12852, 8, 20682, 19002, 311, 3619, 3823, 4221, 13, 69778, 2997, 11850, 41760, 7085, 1093, 364, 56893, 279, 15994, 518, 6155, 4109, 1778, 439, 364, 55767, 956, 518, 40615, 17583, 1093, 364, 68, 1326, 16045, 6409, 15112, 660, 4339, 1093, 364, 9336, 22015, 518, 323, 1524, 100166, 11410, 97, 244, 13, 9805, 374, 1401, 25, 364, 17469, 6, 304, 364, 5586, 6201, 6, 6296, 13, 364, 17469, 6, 304, 364, 82, 46851, 6201, 4527, 7429, 11, 5219, 320, 68, 1326, 2637, 220, 2983, 11, 220, 18, 13, 9335, 20, 8, 323, 6300, 90938, 1093, 364, 3648, 4356, 6, 923, 23965, 13]\n",
      "Natural Language Processing (NLP) enables computers to understand human language. Challenges include handling idioms like 'kick the bucket', contractions such as 'won't', abbreviations like 'e.g.', hyphenated words like 'well-known', and even emojis ü§ñ. Context is key: 'bank' in 'river bank' vs. 'bank' in 'savings bank'. Also, numbers (e.g., 42, 3.1415) and proper nouns like 'New York' add complexity.\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# cl100k_base is used by many newer OpenAI models\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "ids = enc.encode(text)\n",
    "tokens = [enc.decode([i]) for i in ids] # approximate token display\n",
    "print(tokens) # byte-level BPE pieces; robust to any unicode\n",
    "print(ids)\n",
    "print(enc.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5c062a-f679-447c-b17f-b90ba1ebdf55",
   "metadata": {},
   "source": [
    "### T5 (SentencePiece Unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2ab3733-dbe4-43df-92a5-80a9ad1bcdef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‚ñÅTranslat', 'e', '‚ñÅEnglish', '‚ñÅto', '‚ñÅGerman', ':', '‚ñÅThe', '‚ñÅtoken', 'ization', '‚ñÅis']\n",
      "Die tokenization ist robust.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "name = \"t5-small\"\n",
    "tok = AutoTokenizer.from_pretrained(name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(name)\n",
    "\n",
    "text = \"Translate English to German: The tokenization is robust.\"\n",
    "enc = tok(text, return_tensors=\"pt\")\n",
    "out = model.generate(**enc, max_new_tokens=20)\n",
    "\n",
    "print(tok.convert_ids_to_tokens(enc[\"input_ids\"][0][:10]))  \n",
    "print(tok.decode(out[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
